# setuphadoop.sh

```shell
#!/bin/bash
# Run this only on first master node.

umask 0022

. /etc/profile.d/zf-hadoop.sh

this="${BASH_SOURCE-$0}"
bin=$(cd -P -- "$(dirname -- "${this}")" >/dev/null && pwd -P)

# let's locate libexec...
if [[ -n "${HADOOP_HOME}" ]]; then
  HADOOP_DEFAULT_LIBEXEC_DIR="${HADOOP_HOME}/libexec"
else
  HADOOP_DEFAULT_LIBEXEC_DIR="${bin}/../libexec"
fi

HADOOP_LIBEXEC_DIR="${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR}"

if [[ -f "${HADOOP_LIBEXEC_DIR}/hdfs-config.sh" ]]; then
  . "${HADOOP_LIBEXEC_DIR}/hdfs-config.sh"
else
  echo "ERROR: Cannot execute ${HADOOP_LIBEXEC_DIR}/hdfs-config.sh." 2>&1
  exit 1
fi

if [ -z "${JAVA_HOME}" ]
then
  echo "Environment variable JAVA_HOME not set, exited!" && exit 1
else
  echo "Using Environment variable JAVA_HOME : ${JAVA_HOME}"
fi

if [ -z "${NAME_NODE_LIST}" ]
then
  echo "Environment variable NAME_NODE_LIST not set, exited!" && exit 1
else
  echo "Using Environment variable NAME_NODE_LIST : ${NAME_NODE_LIST}"
fi

if [ -z "${DATA_NODE_LIST}" ]
then
  echo "Environment variable DATA_NODE_LIST not set, exited!" && exit 1
else
  echo "Using Environment variable DATA_NODE_LIST : ${DATA_NODE_LIST}"
fi

if [ -z "${JOURNAL_NODE_LIST}" ]
then
  echo "Environment variable JOURNAL_NODE_LIST not set, exited!" && exit 1
else
  echo "Using Environment variable JOURNAL_NODE_LIST : ${JOURNAL_NODE_LIST}"
fi

if [ -z "${ZK_NODE_LIST}" ]
then
  echo "Environment variable ZK_NODE_LIST not set, exited!" && exit 1
else
  echo "Using Environment variable ZK_NODE_LIST : ${ZK_NODE_LIST}"
fi

if [ -z "${DFS_DATA_DIR}" ]
then
  DFS_DATA_DIR=/var/lib/hadoop/data
  echo "Environment variable DFS_DATA_DIR not set, using default value '${DFS_DATA_DIR}'"
else
  echo "Using Environment variable DFS_DATA_DIR : ${DFS_DATA_DIR}"
fi

if [ -z "${DFS_NAME_DIR}" ]
then
  DFS_NAME_DIR=/var/lib/hadoop/name
  echo "Environment variable DFS_NAME_DIR not set, using default value '${DFS_NAME_DIR}'"
else
  echo "Using Environment variable DFS_NAME_DIR : ${DFS_NAME_DIR}"
fi

if [ -z "${DFS_JOURNAL_DIR}" ]
then
  DFS_JOURNAL_DIR=/var/lib/hadoop/journal
  echo "Environment variable DFS_JOURNAL_DIR not set, using default value '${DFS_JOURNAL_DIR}'"
else
  echo "Using Environment variable DFS_JOURNAL_DIR : ${DFS_JOURNAL_DIR}"
fi

if [ -z "${YARN_LOCAL_DIR}" ]
then
  YARN_LOCAL_DIR=/var/lib/hadoop/yarn
  echo "Environment variable YARN_LOCAL_DIR not set, using default value '${YARN_LOCAL_DIR}'"
else
  echo "Using Environment variable YARN_LOCAL_DIR : ${YARN_LOCAL_DIR}"
fi

if [ ! -e "$DFS_DATA_DIR" ]; then
   /usr/bin/install -d -o hdfs -g hadoop -m 0755  $DFS_DATA_DIR
else
   chown -R hdfs:hadoop $DFS_DATA_DIR
fi

if [ ! -e "$DFS_NAME_DIR" ]; then
   /usr/bin/install -d -o hdfs -g hadoop -m 0755  $DFS_NAME_DIR
else
   chown -R hdfs:hadoop $DFS_NAME_DIR
fi

if [ ! -e "$DFS_JOURNAL_DIR" ]; then
   /usr/bin/install -d -o hdfs -g hadoop -m 0755  $DFS_JOURNAL_DIR
else
   chown -R hdfs:hadoop $DFS_JOURNAL_DIR
fi

if [ ! -e "$YARN_LOCAL_DIR" ]; then
   /usr/bin/install -d -o yarn -g hadoop -m 0755  $YARN_LOCAL_DIR
else
   chown -R yarn:hadoop $YARN_LOCAL_DIR
fi

YARN_CLASSPATH=$("${HADOOP_HDFS_HOME}/bin/hadoop" classpath)
YARN_CLASSPATH_TMP=`echo ${YARN_CLASSPATH} | perl -lne 'print quotemeta'`
JAVA_HOME_TMP=`echo ${JAVA_HOME} | perl -lne 'print quotemeta'`
DFS_DATA_DIR_TMP=`echo ${DFS_DATA_DIR} | perl -lne 'print quotemeta'`
DFS_NAME_DIR_TMP=`echo ${DFS_NAME_DIR} | perl -lne 'print quotemeta'`
DFS_JOURNAL_DIR_TMP=`echo ${DFS_JOURNAL_DIR} | perl -lne 'print quotemeta'`
YARN_LOCAL_DIR_TMP=`echo ${YARN_LOCAL_DIR} | perl -lne 'print quotemeta'`

cd ${HADOOP_CONF_DIR}

sed -i "s/\${YARN_CLASSPATH}/${YARN_CLASSPATH_TMP}/g" yarn-site.xml
sed -i "s/\${DFS_DATA_DIR}/${DFS_DATA_DIR_TMP}/g" hdfs-site.xml
sed -i "s/\${DFS_NAME_DIR}/${DFS_NAME_DIR_TMP}/g" hdfs-site.xml
sed -i "s/\${DFS_JOURNAL_DIR}/${DFS_JOURNAL_DIR_TMP}/g" hdfs-site.xml
sed -i "s/\${YARN_LOCAL_DIR}/${YARN_LOCAL_DIR_TMP}/g" yarn-site.xml
sed -i "s/\${JAVA_HOME}/${JAVA_HOME_TMP}/g" hadoop-env.sh

IFS=','
ARR=(${NAME_NODE_LIST})

for s in ${!ARR[@]}
do
  NODENUM=`expr $s + 1`
  sed -i "s/\${NAMENODE${NODENUM}}/${ARR[s]}/g" hdfs-site.xml
  sed -i "s/\${NAMENODE${NODENUM}}/${ARR[s]}/g" yarn-site.xml
  sed -i "s/\${NAMENODE${NODENUM}}/${ARR[s]}/g" core-site.xml
  sed -i "s/\${NAMENODE${NODENUM}}/${ARR[s]}/g" mapred-site.xml
done

IFS=','
ARR=(${DATA_NODE_LIST})

cat /dev/null > workers
for s in ${!ARR[@]}
do
  echo "${ARR[s]}" >> workers  
done

IFS=','
ARR=(${JOURNAL_NODE_LIST})

JOURNAL_NODE_STR=''
for s in ${!ARR[@]}
do
  JOURNAL_NODE_STR="${JOURNAL_NODE_STR};${ARR[s]}:8485"
done
JOURNAL_NODE_STR=${JOURNAL_NODE_STR:1}

sed -i "s/\${JOURNAL_NODE_LIST}/${JOURNAL_NODE_STR}/g" hdfs-site.xml

IFS=','
ARR=(${ZK_NODE_LIST})

ZK_CONNECT_STR=''
for s in ${!ARR[@]}
do
  ZK_CONNECT_STR="${ZK_CONNECT_STR},${ARR[s]}:2181"
done
ZK_CONNECT_STR=${ZK_CONNECT_STR:1}

sed -i "s/\${ZK_CONNECT_STR}/${ZK_CONNECT_STR}/g" core-site.xml
sed -i "s/\${ZK_CONNECT_STR}/${ZK_CONNECT_STR}/g" yarn-site.xml

echo "Copying config files to remote hosts ..."

NAMENODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -namenodes 2>/dev/null)
LOCALHOST=$(hostname)
unset IFS

for worker in ${NAMENODES}
do
	 if [[ "${worker}" != "${LOCALHOST}" ]]; then
	    scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml hadoop-env.sh ${worker}:${HADOOP_CONF_DIR}/ 2>&1 | sed "s/^/$worker: /"
	 fi
done

#---------------------------------------------------------
# datanodes (using default workers file)
HADOOP_WORKER_NAMES=$(sed 's/#.*$//;/^$/d' "${HADOOP_CONF_DIR}/workers")

for worker in ${HADOOP_WORKER_NAMES}
do
   if [[ "${worker}" != "${LOCALHOST}" ]]; then
	    scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml hadoop-env.sh ${worker}:${HADOOP_CONF_DIR}/ 2>&1 | sed "s/^/$worker: /"
	fi
done

exit 0
```

